{"cells":[{"cell_type":"markdown","source":["## **License and Usage Notice**\n","---\n","\n","**MedLiT-seed** and all accompanying source code in this notebook are released under the **GNU Affero General Public License v3.0 (AGPL-3.0)**.\n","\n","You are free to:\n","- **Use** the code,\n","- **Modify** it,\n","- **Redistribute** it,\n","- **Build upon** it for academic or commercial purposes,\n","\n","**provided that** any distributed or publicly deployed derivative work **must also be released under the AGPL-3.0** and must provide end users with access to the corresponding source code, including any modifications.\n","\n","This includes cases where the code is run as part of a network-accessible service (e.g., APIs, web applications, inference servers), as required by the **Affero clause**.\n","\n","Please note:\n","- The MedLiT-seed model weights are provided strictly for **research and educational** use, and may be subject to additional terms from the hosting institution.\n","- This notebook loads pre-trained weights from a user-provided directory via the variable `weight_path`. You are responsible for ensuring that the model weights you download and use comply with any applicable licensing or data-use restrictions.\n","\n","By using this notebook, you acknowledge that you have read and understood the license terms.\n","\n","For details, see: https://www.gnu.org/licenses/agpl-3.0.en.html\n"],"metadata":{"id":"aVSxizymwbHv"},"id":"aVSxizymwbHv"},{"cell_type":"markdown","source":["---"],"metadata":{"id":"5OH5NHB_xdYF"},"id":"5OH5NHB_xdYF"},{"cell_type":"code","execution_count":null,"id":"cHz0Z_RBIWNY","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13358,"status":"ok","timestamp":1759395656728,"user":{"displayName":"j.h. tan","userId":"00296049209347528429"},"user_tz":-480},"id":"cHz0Z_RBIWNY","outputId":"1d9a3d84-7d31-4c3e-803e-08fccb92eda9","collapsed":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting medmnist\n","  Downloading medmnist-3.0.2-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from medmnist) (1.6.1)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from medmnist) (0.25.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from medmnist) (4.67.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from medmnist) (11.3.0)\n","Collecting fire (from medmnist)\n","  Downloading fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from medmnist) (2.8.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from medmnist) (0.23.0+cu126)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->medmnist) (3.1.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->medmnist) (2025.2)\n","Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (1.16.2)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (3.5)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (2025.9.9)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (25.0)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->medmnist) (0.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->medmnist) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->medmnist) (3.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (1.13.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->medmnist) (3.4.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->medmnist) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->medmnist) (3.0.2)\n","Downloading medmnist-3.0.2-py3-none-any.whl (25 kB)\n","Downloading fire-0.7.1-py3-none-any.whl (115 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: fire, medmnist\n","Successfully installed fire-0.7.1 medmnist-3.0.2\n"]}],"source":["!pip install medmnist"]},{"cell_type":"markdown","id":"d9f9a509","metadata":{"id":"d9f9a509"},"source":["## **1. Import the necessary libraries**\n","---"]},{"cell_type":"code","execution_count":null,"id":"e49ec765","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15998,"status":"ok","timestamp":1759395672718,"user":{"displayName":"j.h. tan","userId":"00296049209347528429"},"user_tz":-480},"id":"e49ec765","outputId":"fee8b8c0-d26b-461a-90a8-dba650605e96"},"outputs":[{"name":"stdout","output_type":"stream","text":["Versions of key libraries\n","---\n","torch:       2.5.1\n","numpy:       2.0.1\n","matplotlib:  3.10.3\n"]}],"source":["import einops as ein\n","import numpy as np\n","import math\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import os\n","import pandas as pd\n","import random\n","import torch\n","import shutil\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","from medmnist import INFO, Evaluator\n","from medmnist import (\n","    OrganCMNIST, BloodMNIST, DermaMNIST, OrganAMNIST, OrganSMNIST,\n","    PathMNIST, PneumoniaMNIST, RetinaMNIST, BreastMNIST, TissueMNIST,\n","    OCTMNIST, ChestMNIST\n",")\n","from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n","from torchvision.transforms import v2\n","from torchvision.utils import make_grid\n","from torch.distributions.normal import Normal\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from google.colab import drive\n","\n","\n","print(\"Versions of key libraries\")\n","print(\"---\")\n","print(\"torch:      \", torch.__version__)\n","print(\"numpy:      \", np.__version__)\n","print(\"matplotlib: \", matplotlib.__version__)\n","\n","\n","def setup_seed(seed=42):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","id":"4390ba07","metadata":{"id":"4390ba07"},"source":["## **2. Hyperparameter setup**\n","---"]},{"cell_type":"code","execution_count":null,"id":"9304e807","metadata":{"id":"9304e807"},"outputs":[],"source":["class Parser():\n","    def __init__(self):\n","\n","        self.seed         = 42         # Deterministic\n","\n","        # Input setup\n","        # ----------------------------------\n","        self.img_size     = (256, 256)    # image size\n","        self.patch_size   = 16            # patch size\n","        self.num_patches  = (self.img_size[0] // self.patch_size) ** 2 + 1\n","        self.patch_length = (self.patch_size ** 2) * 3\n","        self.num_classes  = 12\n","        self.batch_size   = 128\n","\n","        # Encoder setup\n","        # ----------------------------------\n","        self.num_heads    = 6\n","        self.num_experts  = 3\n","        self.k            = 2\n","        self.embed_size   = 216\n","        self.hidden_size  = 27\n","        self.num_groups   = 3\n","        self.dropout_rate = 0.1\n","        self.out_proj     = 216\n","\n","        self.num_layers   = 9\n","        self.num_blocks   = 3\n","        self.hidden_ratio = 3\n","        self.lyrs_per_block  = self.num_layers / self.num_blocks\n","\n","\n","args    = Parser()\n","device  = 'cuda' if torch.cuda.is_available() else 'cpu'\n","setup_seed(args.seed)"]},{"cell_type":"markdown","id":"165ec988-32c9-4b7c-bb6d-4471bf474f2e","metadata":{"id":"165ec988-32c9-4b7c-bb6d-4471bf474f2e"},"source":["## **3. Modules required to build Mixture-of-Experts layer**\n","---"]},{"cell_type":"code","execution_count":null,"id":"ad9ce342-0e8b-4d2c-adfd-22aa65e47980","metadata":{"id":"ad9ce342-0e8b-4d2c-adfd-22aa65e47980"},"outputs":[],"source":["class FeedForward(nn.Module):\n","    def __init__(self,\n","                 embed_size,\n","                 hidden_size,\n","                 output_size,\n","                 dropout_rate):\n","        super().__init__()\n","        self.FeedForward  = nn.Sequential(nn.Linear(in_features=embed_size,\n","                                                    out_features=hidden_size),\n","                                          nn.ReLU(),\n","                                          nn.Linear(in_features=hidden_size,\n","                                                    out_features=output_size),\n","                                          nn.Dropout(dropout_rate))\n","\n","    def forward(self, x):\n","        return self.FeedForward(x)\n","                                    # 'x' has a shape of (batchSize, num_embeds, output_size)"]},{"cell_type":"code","execution_count":null,"id":"0d1b83a3-ad60-43fe-b274-606923b8dca0","metadata":{"id":"0d1b83a3-ad60-43fe-b274-606923b8dca0"},"outputs":[],"source":["class FFNSwiGLU(nn.Module):\n","    def __init__(self,\n","                 embed_size,\n","                 hidden_size,\n","                 output_size,\n","                 dropout_rate=0.0,\n","                 V=None,\n","                 W2=None):\n","        super(FFNSwiGLU, self).__init__()\n","\n","\n","        self.W      = nn.Linear(in_features=embed_size,\n","                                out_features=hidden_size)\n","\n","        self.V      = V\n","        self.W2     = W2\n","        if self.V is None:\n","            self.V  = nn.Linear(in_features=embed_size,\n","                                out_features=hidden_size)\n","        if self.W2 is None:\n","            self.W2 = nn.Linear(in_features=hidden_size,\n","                                out_features=output_size)\n","        self.dropout= nn.Dropout(dropout_rate)\n","\n","    def forward(self, x):\n","        xW        = self.W(x)\n","        xV        = self.V(x)\n","\n","        output    = self.W2(F.silu(xW) * xV)\n","\n","        return self.dropout(output)"]},{"cell_type":"code","execution_count":null,"id":"643fb0f7-e327-49e3-b8c8-455f8ef67ae0","metadata":{"id":"643fb0f7-e327-49e3-b8c8-455f8ef67ae0"},"outputs":[],"source":["class DataDispatcher(object):\n","    def __init__(self,\n","                 num_experts=4):\n","\n","        self.num_experts      = num_experts\n","        self.gates            = None\n","        self.batch_index      = None\n","        self.part_sizes       = None\n","        self.score_for_expert = None\n","\n","    def dispatch(self,\n","                 data,\n","                 gates):\n","                                            # 'data' has a shape of (batch_size, length)\n","                                            # 'gates' has a shape of (batch_size, num_experts)\n","        self.gates            = gates\n","        experts               = torch.nonzero(self.gates)\n","                                            # Get the index of the expert to which the input 'data'\n","                                            # will be directed to\n","                                            # the shape of `expert` is (batch_size * k, 2)\n","        (sorted_experts,\n","        sorted_experts_idx)  = experts.sort(dim=0)\n","                                            # Sort the `expert` along the column direction,\n","                                            # so that the rows (in `data`) that shall be directed to\n","                                            # specific expert can be put together for later slicing\n","\n","                                            # for `sorted_expert_idx`, what matters is the second column,\n","                                            # where the index tells where the value comes from\n","                                            # in the second column of `sorted_expert`\n","\n","                                            # both `sorted_experts` and `sorted_experts_idx` has\n","                                            # a shape of (batch_size * k, 2)\n","\n","        (_, expert_index)     = sorted_experts.split(1, dim=1)\n","                                            # Split `sorted_experts` by columns and take only the second columns\n","                                            # `expert_index` has a shape of (batch_size * k, 1)\n","\n","        self.batch_index      = experts[sorted_experts_idx[:, 1], 0]\n","                                            # Get the index of row (from `data`) that will go to\n","                                            # the expert specified in 'expert_index'\n","                                            # `batch_index` has a shape of batch_size * k (1d array)\n","\n","        self.part_sizes       = (self.gates > 0).sum(0).tolist()\n","                                            # Count the number of inputs that will go to each expert\n","                                            # 'part_sizes' is a list of the number of rows that will\n","                                            # go to each expert\n","\n","                                            # Based on the indices in `batch_index`, we retrieve\n","                                            # the corresponding rows in `gates`, as a result\n","                                            # `gates_retrieved` has a shape of (batch_size * k, num_expert)\n","        gates_retrieved       = self.gates[self.batch_index]\n","        self.score_for_expert = torch.gather(input=gates_retrieved,\n","                                           dim=1,\n","                                           index=expert_index)\n","                                            # Then for each row, take the score that will be used for calculation\n","                                            # for the expert specified in `expert_index`\n","                                            # `score_for_expert` has a shape of (batch_size * k, 1)\n","\n","                                            # Prepare the data to be dispatched to each expert\n","        data_retrieved        = data[self.batch_index]\n","                                            # `data_retrieved` has a shape of (batch_size * k, length)\n","\n","                                            # Split the data into a list of `num_expert` tensors\n","        data_dispatched       = torch.split(data_retrieved,\n","                                          self.part_sizes,\n","                                          dim=0)\n","\n","        return data_dispatched\n","\n","\n","    def combine(self,\n","                expert_output,\n","                multiply_by_gates=True):\n","                                            # `expert_output` is a list of torch tensors. The amount\n","                                            # of torch tensors is equal to `num_experts`\n","        merged          = torch.cat(expert_output, dim=0)\n","\n","        if multiply_by_gates:\n","            merged      = merged.mul(self.score_for_expert)\n","                                            # `merged` has a shape of (batch_size * k, length)\n","        zeros           = torch.zeros(self.gates.size(0),\n","                                    expert_output[0].size(1),\n","                                    device=merged.device)\n","                                            # create a tensor that has the same shape of input data\n","                                            # NOTE: compared to David Rau's implementation,\n","                                            # we did not set 'requires_grad' to True at here\n","\n","        combined        = zeros.index_add(dim=0,\n","                                        index=self.batch_index,\n","                                        source=merged)\n","                                            # for each index in `batch_index`, retrieve the rows in `merged` by the index, add those rows,\n","                                            # and put the output back into the row specified by the index\n","\n","        return combined\n","                                            # both `zeros` and `combined` have a shape of\n","                                            # (batch_size, length)"]},{"cell_type":"code","execution_count":null,"id":"aaa7e771-347c-48b7-9ca7-4b9d3e45a74b","metadata":{"id":"aaa7e771-347c-48b7-9ca7-4b9d3e45a74b"},"outputs":[],"source":["class MixtureOfExperts(nn.Module):\n","    def __init__(self,\n","                 embed_size,                # embed_length\n","                 hidden_size,\n","                 dropout_rate=0.0,\n","                 num_experts=4,\n","                 k=2,\n","                 expert_type='FFN',\n","                 noisy_gating=True,\n","                 noise_epsilon=1e-2,\n","                 loss_coef=1e-2):\n","      super().__init__()\n","      self.num_experts  = num_experts\n","      self.k            = k\n","      self.embed_size   = embed_size\n","      self.hidden_size  = hidden_size\n","      self.dropout_rate = dropout_rate\n","      self.noisy_gating = noisy_gating\n","      self.noise_epsilon= noise_epsilon\n","      self.loss_coef    = loss_coef\n","      self.H_x          = None\n","      self.w_gate       = nn.Parameter(torch.randn(embed_size,\n","                                                   num_experts),\n","                                       requires_grad=True)\n","                                            # `w_gate` has a shape of (embed_size, num_experts)\n","      self.w_noise      = nn.Parameter(torch.randn(embed_size,\n","                                                   num_experts),\n","                                       requires_grad=True)\n","                                            # `w_noise` has a shape of (embed_size, num_experts)\n","\n","      self.dispatcher   = DataDispatcher(num_experts=self.num_experts)\n","      self.expert_type  = expert_type\n","\n","      if self.expert_type == 'FFN':\n","        self.experts      = nn.ModuleList([FeedForward(self.embed_size,\n","                                                       self.hidden_size,\n","                                                       self.embed_size,\n","                                                       self.dropout_rate) for i in range(self.num_experts)])\n","      elif self.expert_type == 'FFNSwiGLU':\n","        self.experts      = nn.ModuleList([FFNSwiGLU(self.embed_size,\n","                                                     self.hidden_size,\n","                                                     self.embed_size,\n","                                                     self.dropout_rate) for i in range(self.num_experts)])\n","      elif self.expert_type == 'FFNSwiGLUShared':\n","        V                 = nn.Linear(in_features=self.embed_size,\n","                                      out_features=self.hidden_size)\n","        W2                = nn.Linear(in_features=self.hidden_size,\n","                                      out_features=self.embed_size)\n","\n","        self.experts      = nn.ModuleList([FFNSwiGLU(self.embed_size,\n","                                                     self.hidden_size,\n","                                                     self.embed_size,\n","                                                     self.dropout_rate,\n","                                                     V,\n","                                                     W2) for i in range(self.num_experts)])\n","\n","      self.softplus     = nn.Softplus()\n","      self.softmax      = nn.Softmax(dim=1)\n","      self.normal       = Normal(0.0, 1.0)\n","\n","      assert self.k <= self.num_experts\n","\n","\n","    # ----------------------\n","    def cv_squared(self, x):\n","                                            # calculate the squared coefficient of variation\n","                                            # x has to be a 1d array\n","      eps       = 1e-10\n","\n","      if x.shape[0] == 1:\n","          return torch.tensor([0],\n","                              device=x.device,\n","                              dtype=x.dtype)\n","                                            # The case where the array has only 1 value\n","      else:\n","          return x.var() /( x.mean()**2 + eps)\n","\n","\n","    #----------------------\n","    def to_2d(self, x):\n","      return ein.rearrange(x, 'b r c -> (b r) c')\n","\n","\n","    def to_3d(self, x, r):\n","      return ein.rearrange(x, '(b r) c -> b r c', r=r)\n","\n","\n","\n","    # ----------------------\n","    def prob_in_top_k(self,\n","                      x_W_g,\n","                      logits,\n","                      noise_std,\n","                      top_logits):\n","      batch             = x_W_g.size(0)\n","      m                 = top_logits.size(1)\n","      top_logits_flatten= top_logits.flatten()\n","                                          # `top_logits_flatten` has a shape of\n","                                          # (batch_size * num_patches * k,)\n","\n","      threshold_positions_if_in = torch.arange(batch,\n","                                               device=x_W_g.device) * m + self.k\n","                                          # `threshold_position_if_in` has a shape of\n","                                          # (batch_size * num_patches,)\n","      threshold_if_in             = torch.unsqueeze(torch.gather(top_logits_flatten,\n","                                                                 0,\n","                                                                 threshold_positions_if_in),\n","                                                    1)\n","      is_in                       = torch.gt(logits, threshold_if_in)\n","                                          # `is_in` has a shape of\n","                                          # (batch_size * num_patches, num_experts)\n","\n","      threshold_positions_if_out  = threshold_positions_if_in - 1\n","      threshold_if_out            = torch.unsqueeze(torch.gather(top_logits_flatten,\n","                                                                 0,\n","                                                                 threshold_positions_if_out),\n","                                                    1)\n","\n","      sub_in        = x_W_g - threshold_if_in\n","      sub_out       = x_W_g - threshold_if_out\n","      sub           = torch.where(is_in, sub_in, sub_out)\n","                                          # `sub_in`, `sub_out` and `sub` have a shape of\n","                                          # (batch_size * num_patches, num_experts)\n","\n","      prob_of_sub   = self.normal.cdf(sub/noise_std)\n","                                          # `prob_of_sub` has a shape of\n","                                          # (batch_size * num_patches, num_experts)\n","\n","      return prob_of_sub\n","\n","    # ----------------------\n","    def noisy_top_k_gating(self,\n","                           x,\n","                           train=False,\n","                           noise_epsilon=1e-2):\n","                                            # in this implementation, the shape of x\n","                                            # is (batch_size*num_patches, embed_length)\n","      x_W_g           = x @ self.w_gate\n","                                            # `x_W_g` has a shape of (batch_size * num_patches, num_experts)\n","\n","      if self.noisy_gating and train:\n","          x_W_noise   = x @ self.w_noise\n","                                            # `x_W_noise` has a shape of (batch_size * num_patches, num_experts)\n","          noise_std   = self.softplus(x_W_noise) + noise_epsilon\n","          logits      = x_W_g + torch.randn_like(x_W_g) * noise_std\n","      else:\n","          logits      = x_W_g\n","                                            # `noise_std`, `logits`\n","                                            # all have a shape of (batch_size * num_patches, num_experts)\n","      self.H_x        = logits\n","\n","      (top_logits,\n","       top_indices)   = logits.topk(min(self.k + 1, self.num_experts), dim=1)\n","                                            # `top_logits` is required for calculating load-balacing loss\n","                                            # `top_logits` and `top_indices` have a shape of\n","                                            # (batch_size * num_patches, k + 1)\n","      top_k_logits    = top_logits[:, :self.k]\n","      top_k_indices   = top_indices[:, :self.k]\n","      top_k_gates     = self.softmax(top_k_logits)\n","                                            # `top_k_logits`, `top_k_indices` and `top_k_gates` have a shape of\n","                                            # (batch_size * num_patches, k)\n","      zeros           = torch.zeros_like(logits)\n","                                            # NOTE: compared to David Rau's implementation,\n","                                            # we did not set 'requires_grad' to True at here\n","      gates           = zeros.to(x.dtype).scatter(1,\n","                                                  top_k_indices,\n","                                                  top_k_gates)\n","                                             # `zeros` and `gates` have a shape of\n","                                             # (batch_size * num_patches, num_experts)\n","\n","      if self.noisy_gating and train and (self.num_experts - self.k > 0):\n","          load        = self.prob_in_top_k(x_W_g,\n","                                           logits,\n","                                           noise_std,\n","                                           top_logits).sum(0)\n","                                            # `load` has a shape of (num_experts,)\n","      else:\n","          load        = (gates > 0).sum(0)\n","          load        = load.float()\n","                                            # `load` has a shape of (num_experts,)\n","\n","      return (gates, load)\n","\n","\n","    # ----------------------\n","    def forward(self,\n","                x,\n","                train=False,\n","                previous_loss=0):\n","                                            # `x` has a shape of\n","                                            # (batch_size, num_patches, embed_length)\n","\n","      r              = x.shape[1]           # required for self.to_3d()\n","      x_2d           = self.to_2d(x)\n","      (gates,\n","        load)        = self.noisy_top_k_gating(x_2d,\n","                                               train,\n","                                               noise_epsilon=self.noise_epsilon)\n","\n","      expert_inputs = self.dispatcher.dispatch(x_2d, gates)\n","      expert_outputs= [self.experts[i](expert_inputs[i]) for i in range(self.num_experts)]\n","      y             = self.dispatcher.combine([expert_outputs[i] for i in range(self.num_experts)])\n","\n","      if train:\n","          importance= gates.sum(0)\n","          loss      = self.loss_coef*(self.cv_squared(importance)+self.cv_squared(load))\n","          loss      = loss+previous_loss\n","\n","          return (self.to_3d(y,r), loss)\n","      else:\n","          return self.to_3d(y,r)\n","\n"]},{"cell_type":"markdown","id":"80498be2","metadata":{"id":"80498be2"},"source":["## **4. Modules and functions required for MAE-ViT Classifier**\n","---"]},{"cell_type":"code","execution_count":null,"id":"dad410d4","metadata":{"id":"dad410d4"},"outputs":[],"source":["# 1.\n","# -------------------------\n","def get_patch_embeddings(images,\n","                         patch_size):\n","\n","    unfold  = F.unfold(images,\n","                       kernel_size=patch_size,\n","                       stride=patch_size)\n","                                      # `unfold` has a shape of\n","                                      # [batch_size, embed_length, num_patches]\n","\n","    embeds  = unfold.transpose(1, 2)\n","                                      # `embeds` has a shape of\n","                                      # [batch_size, num_patches, embed_length]\n","    return embeds\n","\n","# 2.\n","# -------------------------\n","def get_patch_w_class_embed(images,\n","                            patch_size,\n","                            cls='zeros'):\n","    embeds            = get_patch_embeddings(images,\n","                                             patch_size)\n","\n","    if cls == 'zeros':\n","        # cls_embeds    = torch.zeros(embeds.shape[0],\n","        #                             1,\n","        #                             embeds.shape[-1])\n","        cls_embeds = torch.zeros(embeds.shape[0], 1, embeds.shape[-1], device=embeds.device)\n","        embeds        = torch.cat((embeds,cls_embeds),\n","                                  dim=1)\n","                                      # `embeds` has a shape of\n","                                      # [batch_size, num_patches + 1, embed_length]\n","\n","    return embeds\n","\n","\n","# 3.\n","# -------------------------\n","class GQAttentionHead(nn.Module):\n","    def __init__(self,\n","                 head_size,\n","                 embed_size,\n","                 heads_per_group,\n","                 attn_mask=None,\n","                 dropout_p=0.0,\n","                 is_causal=False):\n","        super().__init__()\n","\n","        self.attn_mask= attn_mask\n","        self.dropout_p= dropout_p\n","        self.is_causal= is_causal\n","\n","        self.key      = nn.Linear(in_features=embed_size,\n","                                  out_features=head_size,\n","                                  bias=False)\n","\n","        self.queries  = nn.ModuleList([nn.Linear(in_features=embed_size,\n","                                                 out_features=head_size,\n","                                                 bias=False) for _ in range(heads_per_group)])\n","\n","        self.value    = nn.Linear(in_features=embed_size,\n","                                  out_features=head_size,\n","                                  bias=False)\n","\n","\n","    def forward(self, x):\n","\n","        k               = self.key(x)     # 'k' has a shape of (batchSize, num_embeds, head_size)\n","        v               = self.value(x)\n","\n","        attn_out        = torch.cat([F.scaled_dot_product_attention(q(x),\n","                                                                    k,\n","                                                                    v,\n","                                                                    attn_mask=self.attn_mask,\n","                                                                    dropout_p=self.dropout_p,\n","                                                                    is_causal=self.is_causal) for q in self.queries], dim=-1)\n","                                          # 'attn_out' now has a shape of (batchSize, num_embeds, num_heads*head_size)\n","        return attn_out\n","\n","\n","# 4.\n","# -------------------------\n","class GroupedQueryAttention(nn.Module):\n","    def __init__(self,\n","                 num_heads,\n","                 embed_size,\n","                 dropout_rate,\n","                 num_groups,\n","                 attn_mask=None,\n","                 dropout_p=0.0,\n","                 is_causal=False):\n","        super().__init__()\n","        self.Heads    = nn.ModuleList([GQAttentionHead(embed_size // num_heads,\n","                                                       embed_size,\n","                                                       num_heads // num_groups,     # heads_per_group\n","                                                       attn_mask,\n","                                                       dropout_p,\n","                                                       is_causal) for _ in range(num_groups)])\n","                                    # embed_size // num_heads gives head_size\n","\n","        self.proj     = nn.Linear(in_features=num_heads*(embed_size // num_heads),\n","                                  out_features=embed_size,\n","                                  bias=False)\n","        self.dropout  = nn.Dropout(dropout_rate)\n","\n","                                    # The input 'x' should have a shape of (batchSize, num_embeds, embed_size)\n","    def forward(self, x):\n","        attn_out      = torch.cat([h(x) for h in self.Heads], dim=-1)\n","                                    # 'attn_out' now has a shape of (batchSize, num_embeds, num_heads*head_size)\n","        attn_out      = self.proj(attn_out)\n","                                    # 'attn_out' now has a shape of (batchSize, num_embeds, embed_size)\n","        return self.dropout(attn_out)\n","\n","# 5.\n","# -------------------------\n","class TransformerMoEBlock(nn.Module):\n","    def __init__(self,\n","                 num_heads,\n","                 embed_size,\n","                 hidden_size,\n","                 num_groups,\n","                 num_experts,\n","                 k,\n","                 dropout_rate,\n","                 expert_type='FFN'):\n","        super().__init__()\n","        self.GQAttn               = GroupedQueryAttention(num_heads=num_heads,\n","                                                          embed_size=embed_size,\n","                                                          dropout_rate=dropout_rate,\n","                                                          num_groups=num_groups)\n","        self.MoE                  = MixtureOfExperts(embed_size=embed_size,\n","                                                     hidden_size=hidden_size,\n","                                                     dropout_rate=dropout_rate,\n","                                                     num_experts=num_experts,\n","                                                     k=k,\n","                                                     expert_type=expert_type)\n","\n","        self.LayerNorm1           = nn.LayerNorm(normalized_shape=embed_size)\n","        self.LayerNorm2           = nn.LayerNorm(normalized_shape=embed_size)\n","\n","                                    # The input 'x' should have a shape of (batchSize, num_embeds, embed_size)\n","    def forward(self,\n","                inputs):\n","        x               = inputs[0]\n","        train           = inputs[1]\n","        previous_loss   = inputs[2]\n","\n","        x       = self.LayerNorm1(x)\n","        x       = x + self.GQAttn(x)\n","                                    # The output 'x' has a shape of (batchSize, num_embeds, embed_size\n","        if not train:\n","            x       = x + self.MoE(self.LayerNorm2(x))\n","                                    # The output 'x' has a shape of (batchSize, num_embeds, embed_size)\n","            return (x, train, 0)\n","        else:\n","            moe_out = self.MoE(self.LayerNorm2(x),\n","                                   train,\n","                                   previous_loss)\n","            x       = x + moe_out[0]\n","            return (x, train, moe_out[1])\n","                                    # 'x[0]' has a shape of (batchSize, num_embeds, embed_size)\n","                                    # 'x[1] is the loss\n","\n","\n","# 6.\n","# -------------------------\n","                                        # The input to `maskEncoder` is of shape\n","                                        # (batch_size, num_patches, patch_length)\n","class MaskEncoder(nn.Module):\n","    def __init__(self,\n","                 num_heads,\n","                 num_layers,\n","                 num_patches,\n","                 patch_length,\n","                 embed_size,\n","                 hidden_size,\n","                 num_groups,\n","                 num_experts,\n","                 k,\n","                 out_proj,\n","                 dropout_rate,\n","                 num_blocks):\n","        super().__init__()\n","        self.num_blocks       = num_blocks\n","        self.num_layers       = num_layers\n","        self.num_heads        = num_heads\n","        self.embed_size       = embed_size\n","        self.hidden_size      = hidden_size\n","        self.num_groups       = num_groups\n","        self.num_experts      = num_experts\n","        self.k                = k\n","        self.dropout_rate     = dropout_rate\n","        self.MaskEmbed        = nn.Parameter(torch.randn(embed_size),\n","                                             requires_grad=True)\n","        self.Proj             = nn.Linear(in_features=patch_length,\n","                                          out_features=embed_size,\n","                                          bias=False)\n","        self.PositionEmbeds   = nn.Embedding(num_embeddings=num_patches,\n","                                             embedding_dim=embed_size)\n","\n","        self.TransformerBlocks= self.create_transformer_blocks()\n","        self.LayerNorm        = nn.LayerNorm(normalized_shape=embed_size)\n","        self.OutProj          = nn.Linear(in_features=embed_size,\n","                                          out_features=out_proj,\n","                                          bias=False)\n","        self.register_buffer('full_pos_idx',\n","                             torch.arange(num_patches))\n","\n","\n","\n","    def create_transformer_blocks(self,\n","                                  hidden_ratio=args.hidden_ratio):\n","        num_experts           = self.num_experts\n","        k                     = self.k\n","        lyrs_per_block        = math.ceil(self.num_layers/self.num_blocks)\n","        blocks                = nn.ModuleList()\n","        max_hidden_size       = self.hidden_size * hidden_ratio\n","\n","        for i in range(self.num_layers):\n","            additional_hidden = int(((self.num_layers-1)-i)*(max_hidden_size-self.hidden_size)/(self.num_layers-1))\n","            blocks.append(TransformerMoEBlock(num_heads=self.num_heads,\n","                                              embed_size=self.embed_size,\n","                                              hidden_size=self.hidden_size+additional_hidden,\n","                                              num_groups=self.num_groups,\n","                                              num_experts=num_experts,\n","                                              k=k,\n","                                              dropout_rate=self.dropout_rate,\n","                                              expert_type='FFNSwiGLUShared'))\n","            if (i+1) % lyrs_per_block == 0:\n","                num_experts   += 1\n","\n","        return nn.Sequential(*blocks)\n","\n","\n","    def forward(self,\n","                x,\n","                moe_train=False,\n","                mae_train=False):\n","        if not mae_train:\n","            x       = self.Proj(x) + self.PositionEmbeds(self.full_pos_idx)\n","\n","            if not moe_train:\n","                return self.LayerNorm(self.TransformerBlocks((x,\n","                                                              moe_train,\n","                                                              0))[0])\n","            else:\n","                y   = self.TransformerBlocks((x,\n","                                              moe_train,\n","                                              0))\n","                return (self.LayerNorm(y[0]),\n","                        y[2])\n","        else:\n","            masked_patches    = x[0]\n","            unmasked_patches  = x[1]\n","            masked_indices    = x[2]\n","            unmasked_indices  = x[3]\n","\n","            unmasked_embeds   = self.Proj(unmasked_patches) + self.PositionEmbeds(unmasked_indices)\n","            y                 = self.TransformerBlocks((unmasked_embeds,\n","                                                       mae_train,\n","                                                       0))\n","            unmasked_embeds   = self.LayerNorm(y[0])\n","\n","            masked_embeds     = self.LayerNorm(self.PositionEmbeds(masked_indices) + self.MaskEmbed)\n","\n","            return (self.OutProj(masked_embeds),\n","                    self.OutProj(unmasked_embeds),\n","                    y[2])                         # 'y[2]' gives us the loss from mixture of experts\n","\n","\n","\n","# 7.\n","# -------------------------\n","class ViTClassifier(nn.Module):\n","    def __init__(self, encoder, embed_size, num_classes):\n","        super().__init__()\n","        self.Encoder = encoder\n","        self.Dropout = nn.Dropout(0.2)\n","        self.Head = nn.Linear(in_features=embed_size, out_features=num_classes, bias=True)\n","\n","    def forward(self, x, moe_train=False):\n","        if moe_train:\n","            (x, l) = self.Encoder(x, moe_train=moe_train)\n","\n","            if torch.isnan(x).any() or torch.isnan(l).any():\n","                raise ValueError(\"Encoder output or auxiliary loss l contains NaN.\")\n","\n","            x = self.Head(self.Dropout(x[:, -1, :]))\n","            return (x, l)\n","\n","        else:\n","            x = self.Encoder(x)\n","            x = self.Head(x[:, -1, :])\n","            return x\n","\n","\n","# 8.\n","# -------------------------\n","                                                      # trainable parameter counting function for pytorch\n","def count_trainable_params(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"]},{"cell_type":"markdown","id":"e763c09e","metadata":{"id":"e763c09e"},"source":["## **5. Build ViT classifier**\n","---"]},{"cell_type":"code","execution_count":null,"id":"9066ea1c","metadata":{"id":"9066ea1c"},"outputs":[],"source":["class FinetuneHead(nn.Module):\n","    def __init__(self, embed_dim, num_classes, task_type=\"multi-class\"):\n","        super().__init__()\n","        self.fc = nn.Linear(embed_dim, num_classes)\n","        self.task_type = task_type\n","\n","    def forward(self, x):\n","        return self.fc(x)\n","\n","\n","def build_model(task_type):\n","    encoder = MaskEncoder(\n","        args.num_heads, args.num_layers, args.num_patches, args.patch_length,\n","        args.embed_size, args.hidden_size, args.num_groups, args.num_experts,\n","        args.k, args.out_proj, args.dropout_rate, args.num_blocks\n","    )\n","\n","    model = ViTClassifier(encoder, args.embed_size, args.num_classes)\n","    model.Head = FinetuneHead(embed_dim=args.embed_size, num_classes=args.num_classes, task_type=task_type)\n","    return model"]},{"cell_type":"markdown","id":"81b9830b","metadata":{"id":"81b9830b"},"source":["## **6. Setup data pipeline**\n","---"]},{"cell_type":"code","execution_count":null,"id":"1320fd6a","metadata":{"id":"1320fd6a"},"outputs":[],"source":["drive.mount('/content/gdrive')\n","\n","data_source       = None\n","weight_path       = '/content/gdrive/My Drive/your_weights_folder'\n","data_path         = 'data'\n","                                            # Create the data directory\n","os.makedirs(data_path, exist_ok=True)\n","\n","if data_source is not None:\n","    shutil.copytree(data_source,\n","                    data_path,\n","                    dirs_exist_ok=True)\n","    data_download = False\n","else:\n","    data_download = True\n","\n","\n","transform = v2.Compose([\n","    v2.Resize(args.img_size),\n","    v2.ToImage(),\n","    v2.ToDtype(torch.float32, scale=True),\n","    v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","])\n","\n","subset_class_map = {\n","    'chest': ChestMNIST,\n","    'path': PathMNIST,\n","    'pneumonia': PneumoniaMNIST,\n","    'retina': RetinaMNIST,\n","    'breast': BreastMNIST,\n","    'tissue': TissueMNIST,\n","    'oct': OCTMNIST,\n","    'organc': OrganCMNIST,\n","    'organs': OrganSMNIST,\n","    'organa': OrganAMNIST,\n","    'blood': BloodMNIST,\n","    'derma': DermaMNIST\n","}"]},{"cell_type":"markdown","id":"fd0250d2-2329-4b8e-9dd8-47a66b8e50b2","metadata":{"id":"fd0250d2-2329-4b8e-9dd8-47a66b8e50b2"},"source":["## **7. Test finetuned model on each subset**\n","---"]},{"cell_type":"code","execution_count":null,"id":"0001fa94","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1121687,"status":"ok","timestamp":1759401773212,"user":{"displayName":"j.h. tan","userId":"00296049209347528429"},"user_tz":-480},"id":"0001fa94","outputId":"7f6cba05-dad7-4925-98fe-7a9feadb95de"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Testing model for subset retina\n","------------------------\n","Model weight: ./all-runs/e216_Ip_IMf_batch256/retina_best_model.pth\n","Using downloaded and verified file: ./data/retinamnist_224.npz\n"]},{"name":"stderr","output_type":"stream","text":["Testing:    : 100%|██████████| 4/4 [00:01<00:00,  2.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","[Test] retina - Accuracy: 0.5975\n","[Test] retina - Loss    : 1.0451\n","[Test] retina - AUC     : 0.8006\n","\n","Testing model for subset chest\n","------------------------\n","Model weight: ./all-runs/e216_Ip_IMf_batch256/chest_best_model.pth\n","Using downloaded and verified file: ./data/chestmnist_224.npz\n"]},{"name":"stderr","output_type":"stream","text":["Testing:    : 100%|██████████| 176/176 [01:19<00:00,  2.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","[Test] chest - Accuracy: 0.9339\n","[Test] chest - Loss    : 0.1737\n","[Test] chest - AUC     : 0.5277\n","\n","Testing model for subset breast\n","------------------------\n","Model weight: ./all-runs/e216_Ip_IMf_batch256/breast_best_model.pth\n","Using downloaded and verified file: ./data/breastmnist_224.npz\n"]},{"name":"stderr","output_type":"stream","text":["Testing:    : 100%|██████████| 2/2 [00:00<00:00,  3.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","[Test] breast - Accuracy: 0.8974\n","[Test] breast - Loss    : 0.4204\n","[Test] breast - AUC     : 0.9175\n","\n","Testing model for subset tissue\n","------------------------\n","Model weight: ./all-runs/e216_Ip_IMf_batch256/tissue_best_model.pth\n","Using downloaded and verified file: ./data/tissuemnist_224.npz\n"]},{"name":"stderr","output_type":"stream","text":["Testing:    : 100%|██████████| 370/370 [02:47<00:00,  2.21it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","[Test] tissue - Accuracy: 0.7207\n","[Test] tissue - Loss    : 0.8173\n","[Test] tissue - AUC     : 0.9379\n","\n","Testing model for subset oct\n","------------------------\n","Model weight: ./all-runs/e216_Ip_IMf_batch256/oct_best_model.pth\n","Using downloaded and verified file: ./data/octmnist_224.npz\n"]},{"name":"stderr","output_type":"stream","text":["Testing:    : 100%|██████████| 8/8 [00:03<00:00,  2.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","[Test] oct - Accuracy: 0.8590\n","[Test] oct - Loss    : 0.4123\n","[Test] oct - AUC     : 0.9744\n","\n","Testing model for subset organs\n","------------------------\n","Model weight: ./all-runs/e216_Ip_IMf_batch256/organs_best_model.pth\n","Using downloaded and verified file: ./data/organsmnist_224.npz\n"]},{"name":"stderr","output_type":"stream","text":["Testing:    : 100%|██████████| 69/69 [00:31<00:00,  2.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","[Test] organs - Accuracy: 0.8161\n","[Test] organs - Loss    : 0.5960\n","[Test] organs - AUC     : 0.9822\n","\n","Testing model for subset organa\n","------------------------\n","Model weight: ./all-runs/e216_Ip_IMf_batch256/organa_best_model.pth\n","Using downloaded and verified file: ./data/organamnist_224.npz\n"]},{"name":"stderr","output_type":"stream","text":["Testing:    : 100%|██████████| 139/139 [01:05<00:00,  2.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","[Test] organa - Accuracy: 0.9646\n","[Test] organa - Loss    : 0.1371\n","[Test] organa - AUC     : 0.9991\n","\n","Testing model for subset organc\n","------------------------\n","Model weight: ./all-runs/e216_Ip_IMf_batch256/organc_best_model.pth\n","Using downloaded and verified file: ./data/organcmnist_224.npz\n"]},{"name":"stderr","output_type":"stream","text":["Testing:    : 100%|██████████| 65/65 [00:30<00:00,  2.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","[Test] organc - Accuracy: 0.9458\n","[Test] organc - Loss    : 0.1797\n","[Test] organc - AUC     : 0.9976\n","\n","Testing model for subset blood\n","------------------------\n","Model weight: ./all-runs/e216_Ip_IMf_batch256/blood_best_model.pth\n","Using downloaded and verified file: ./data/bloodmnist_224.npz\n"]},{"name":"stderr","output_type":"stream","text":["Testing:    : 100%|██████████| 27/27 [00:15<00:00,  1.69it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","[Test] blood - Accuracy: 0.9860\n","[Test] blood - Loss    : 0.0514\n","[Test] blood - AUC     : 0.9993\n","\n","Testing model for subset derma\n","------------------------\n","Model weight: ./all-runs/e216_Ip_IMf_batch256/derma_best_model.pth\n","Using downloaded and verified file: ./data/dermamnist_224.npz\n"]},{"name":"stderr","output_type":"stream","text":["Testing:    : 100%|██████████| 16/16 [00:08<00:00,  1.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","[Test] derma - Accuracy: 0.8299\n","[Test] derma - Loss    : 0.6066\n","[Test] derma - AUC     : 0.9653\n","\n","Testing model for subset pneumonia\n","------------------------\n","Model weight: ./all-runs/e216_Ip_IMf_batch256/pneumonia_best_model.pth\n","Using downloaded and verified file: ./data/pneumoniamnist_224.npz\n"]},{"name":"stderr","output_type":"stream","text":["Testing:    : 100%|██████████| 5/5 [00:02<00:00,  2.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","[Test] pneumonia - Accuracy: 0.8910\n","[Test] pneumonia - Loss    : 0.4483\n","[Test] pneumonia - AUC     : 0.9850\n","\n","Testing model for subset path\n","------------------------\n","Model weight: ./all-runs/e216_Ip_IMf_batch256/path_best_model.pth\n","Using downloaded and verified file: ./data/pathmnist_224.npz\n"]},{"name":"stderr","output_type":"stream","text":["Testing:    : 100%|██████████| 57/57 [00:31<00:00,  1.83it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","[Test] path - Accuracy: 0.9532\n","[Test] path - Loss    : 0.1637\n","[Test] path - AUC     : 0.9976\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["def get_loss_function(task_type):\n","    if task_type == \"multi-label\":\n","        return nn.BCEWithLogitsLoss()\n","    else:\n","        return nn.CrossEntropyLoss()\n","\n","\n","def get_predictions(logits, task_type, threshold=None):\n","    if task_type == \"multi-label\":\n","        if threshold is None:\n","            threshold = 0.5\n","        return (torch.sigmoid(logits) > threshold).float()\n","    else:\n","        return torch.argmax(logits, dim=1)\n","\n","\n","trained_subsets   = [\"chest\",\n","                     \"retina\",\n","                     \"breast\",\n","                     \"tissue\",\n","                     \"oct\",\n","                     \"organs\",\n","                     \"organa\",\n","                     \"organc\",\n","                     \"blood\",\n","                     \"derma\",\n","                     \"pneumonia\",\n","                     \"path\"]\n","\n","subsets           = ['retina',\n","                     'chest',\n","                     'breast',\n","                     'tissue',\n","                     'oct',\n","                     'organs',\n","                     'organa',\n","                     'organc',\n","                     'blood',\n","                     'derma',\n","                     'pneumonia',\n","                     'path']\n","\n","multilabel_subsets= {\"chest\"}\n","threshold         = None                              # for multi-label dataset\n","\n","for subset in subsets:\n","  model_path      = os.path.join(weight_path, (subset + \"_best_model.pth\"))\n","  task_type       = \"multi-label\" if subset in multilabel_subsets else \"multi-class\"\n","\n","  if subset not in trained_subsets:\n","    print(f\"This model was not trained on a \\\"{subset}\\\" subset\")\n","  else:\n","    print(\"\")\n","    print(f\"Testing model for subset {subset}\")\n","    print(\"------------------------\")\n","    print(f\"Model weight: {model_path}\")\n","\n","                                                      # Create test dataloader\n","    dataset_class = subset_class_map[subset]\n","    test_dataset  = dataset_class(split=\"test\",\n","                                  download=data_download,\n","                                  transform=transform,\n","                                  size=224,\n","                                  root=data_path)\n","\n","    test_loader   = DataLoader(test_dataset,\n","                               batch_size=args.batch_size,\n","                               shuffle=False)\n","                                                      # Identify and set no. of labels for a given subset,\n","                                                      # set threshold for prediction function\n","    test_labels   = test_dataset.labels\n","    if task_type == \"multi-label\":\n","        args.num_classes  = test_labels.shape[1]\n","        pos_counts        = test_labels.sum(axis=0)\n","        total             = test_labels.shape[0]\n","        best_threshold    = threshold or torch.tensor([0.25] * args.num_classes).to(device)\n","    else:\n","        args.num_classes  = len(set(test_dataset.labels.flatten().tolist()))\n","        best_threshold    = None\n","\n","                                                      # Build model\n","    model                 = build_model(task_type).to(device)\n","    model.load_state_dict(torch.load(model_path, weights_only=True))\n","    model.eval()\n","\n","                                                      # Run model\n","    criterion                 = get_loss_function(task_type)\n","    test_loss, test_accuracy  = [], []\n","    y_preds, y_trues, y_probs = [], [], []\n","\n","    with torch.no_grad():\n","        for images, labels in tqdm(test_loader, desc=\"Testing:    \"):\n","            x           = get_patch_w_class_embed(images=images,\n","                                                  patch_size=args.patch_size).to(device)\n","            if task_type == \"multi-label\":\n","                labels  = labels.float().to(device)\n","                                                      # `labels`: (batch_size, num_classes)\n","            else:\n","                labels  = labels.squeeze(1).long().to(device) if labels.ndim == 2 else labels.long().to(device)\n","                                                      # `labels`: (batch_size, )\n","\n","            logits      = model(x, moe_train=False)\n","            logits      = logits[0] if isinstance(logits, tuple) else logits\n","                                                      # `logits`: (batch_size, num_classes)\n","            loss        = criterion(logits, labels)\n","            test_loss.append(loss.item())\n","\n","            preds       = get_predictions(logits.detach(), task_type, threshold=best_threshold)\n","                                                      # `preds`: (batch_size, )\n","            probs       = torch.sigmoid(logits) if task_type == \"multi-label\" else torch.softmax(logits, dim=1)\n","                                                      # `probs`: (batch_size, num_classes), AUC calculation\n","            y_preds.append(preds.cpu().numpy())\n","            y_trues.append(labels.cpu().numpy())\n","            y_probs.append(probs.cpu().numpy())\n","\n","    y_preds       = np.concatenate(y_preds)\n","    y_trues       = np.concatenate(y_trues)\n","    y_probs       = np.concatenate(y_probs)\n","    avg_loss      = sum(test_loss) / len(test_loss)\n","    avg_acc       = np.mean(y_preds == y_trues)\n","\n","    if task_type == \"multi-class\":\n","        num_classes = y_probs.shape[1]\n","        if num_classes == 2:\n","                                                      # Binary class: pick probability of positive class\n","            auc = roc_auc_score(y_trues, y_probs[:, 1])\n","        else:\n","            # Multi-class: use OVR\n","            auc = roc_auc_score(y_trues, y_probs, multi_class='ovr', average='macro')\n","    else:\n","        auc = roc_auc_score(y_trues, y_preds, average='macro')\n","\n","    print(\"\")\n","    print(f\"[Test] {subset} - Accuracy: {avg_acc:.4f}\")\n","    print(f\"[Test] {subset} - Loss    : {avg_loss:.4f}\")\n","    print(f\"[Test] {subset} - AUC     : {auc:.4f}\")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"118f64c1-8f31-4b4a-adaf-396a15e4afa4","metadata":{"id":"118f64c1-8f31-4b4a-adaf-396a15e4afa4"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"15pcku2RWlmkyJcKpf15zwLP5j6taZN4C","timestamp":1765092547206},{"file_id":"1qnDFiHJgKAHiu0JYSiTsuI9F_OXhoMe8","timestamp":1759367646553}],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.11"}},"nbformat":4,"nbformat_minor":5}